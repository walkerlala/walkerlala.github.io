<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <!--http://walkerlala.com/-->
        <link rel="icon" type="image/png" href="../site-icon.png">
        <link rel="stylesheet" type="text/css" href="../css/yubinr.css">
        <script src="../js/prettify.js"></script>
        <!-- <script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.1.1.min.js"></script> -->
        <script src="../js/jquery-3.1.1.min.js"></script>
        <meta http-equiv="CACHE-CONTROL" content="NO-CACHE">
        <meta http-equiv="EXPIRES" content="0">
        <meta http-equiv="CONTENT-LANGUAGE" content="en-US">

        <meta name="AUTHOR" content="walkerlala">
        <title>RNN Tutorial -- by walkerlala</title>
    </head>

    <body data-feedly-mini="yes">
        <div id="wrapper">
            <div id="wrap">
                <div id="top">
                    <div class="top_nav">
                        <span class="tab0"><a href="../index.html">Home</a></span>
                        <span class="tab0 active"><a href="../blog.html">Blog</a></span>
                        <span class="tab0"><a href="../archive.html">Archive</a></span>
                        <span class="tab0"><a href="../about.html">About Me</a></span>
                        <span class="tab0"><a href="../download.html">Download</a></span>
                    </div>  <!--top_nav-->
                    <div class="logo">
                        <img align="left" class="logo_img" src="../img/lambda.png">
                        <span class="logo_text">
                            walkerlala
                        </span>
                        <span class="MathJax_SVG" id="MathJax-Element-113-Frame" tabindex="-1" style="text-align: left; font-size: 15px; width: 813px; float: right; margin: 0px 0px 0px 0px; opacity: .4;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="29.497ex" height="2.577ex" viewBox="0 -806.1 12700.1 1109.7" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="1" id="E194-MJMATHI-77" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path stroke-width="1" id="E194-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="E194-MJMATHI-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path stroke-width="1" id="E194-MJMATHI-68" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path stroke-width="1" id="E194-MJMATHI-6F" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path stroke-width="1" id="E194-MJMATHI-75" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="E194-MJMATHI-6C" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path stroke-width="1" id="E194-MJMATHI-73" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path stroke-width="1" id="E194-MJMATHI-66" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path stroke-width="1" id="E194-MJMATHI-67" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path stroke-width="1" id="E194-MJMATHI-65" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path stroke-width="1" id="E194-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="E194-MJMATHI-72" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="E194-MJMATHI-61" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path stroke-width="1" id="E194-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="E194-MJMAIN-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-77" x="0" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-69" x="716" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-74" x="1062" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-68" x="1423" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-6F" x="2000" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-75" x="2485" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-74" x="3058" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-6C" x="3704" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-6F" x="4002" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-73" x="4488" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-73" x="4957" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-6F" x="5712" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-66" x="6197" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-67" x="7033" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-65" x="7513" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-6E" x="7980" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-65" x="8580" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-72" x="9047" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-61" x="9498" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-6C" x="10028" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-69" x="10326" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-74" x="10672" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMATHI-79" x="11033" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMAIN-2E" x="11531" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMAIN-2E" x="11976" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E194-MJMAIN-2E" x="12421" y="0"></use></g></svg></span></span>
                    </div>
                </div>     <!--top-->

                <div id="main">
                    <br>
                    <div><!-- blog content start -->
                        <h1>A Recurrent Neural Network Tutorial</h1>
                        <p>Artificial neural network have been prevalent these days for machine learning and artificial tasks. It was first introduced in the 40s, reborn in the 80s and gained much interest in recent years. The reason why it is not as popular as some other machine learning techniques lies in the fact that training a neural network is computationally hard, and the training algorithms available were not smart enough to decrease the computational cost. Nevertheless, it finally regain many researcher’s interest recently, with the introduction of some fancy new models and parallel design of the algorithms.</p>
                        <p>In this essay, I will focus on one kind of artificial neural network – <strong>Recurrent Neural Network (RNN)</strong>. Essentially there is not so much new in RNN. If you are familiar with the vanilla Feedforward Neural Network, then RNN won’t cause you any trouble. Besides, there have been some another good blogs/articles about RNN, but many of those only tend to provide a crude image of RNN, not detail. So I decide to write one more, and hopefully some other will find something interesting here. This materials are based on what I have learned from papers and online tutorials recently, which may not be 100% correct. Feel free to correct me :)</p>
                        <h2>what is RNN</h2>
                        <p>Basically the architecture of RNN is:</p>
                        <p><div style="display: flex; justify-content: center;"><img src="../img/RNN-basic-arch.png" alt="alt text"></div></p>
                        <center>figure 1</center>
                        It's just like a feedforward neural network. The only difference is that, now the output of the hidden layer is feed to its own input, thus the work <em>recurrent</em>. If you <em>unroll</em> the hidden layer, you get a more clear image:
                        <p><div style="display: flex; justify-content: center;"><img src="../img/RNN-unroll.png" alt="alt text" style="width: 700px; height: 500px;"></div></p>
                        <center>figure 2</center>
                        <p>Note that we use one node for the hidden layer for simplicity(just an abstraction). In real world, the architecture would be much more general and complicated. For example, if we look inside into the hidden layer node, you will see something like this:</p>
                        <p><div style="display: flex; justify-content: center;"><img src="../img/RNN-hidden-detail.png" alt="alt text"></div></p>
                        <center>figure 3</center>
                        <center>(pic-src: http://r2rt.com/static/images/NH_VanillaRNNcell.png)</center>
                        <p>As you can see, it’s very similar to a feed-forward neural network.</p>
                        <h2>What is RNN used for</h2>
                        <p>The goodness of RNN is its ability to learn sequential knowledge. For example, given a sequence of input, we can use RNN to predict the next output:</p>
                        <blockquote>
                            <p>Today is sunny and I am very __</p>
                        </blockquote>
                        <p>With the help of RNN, we can easily predict that the next word of these sequence  is <em>happy</em> or some other similar words. Why can RNN be used for this? Because it record the sequential information of input by the way of its recurrent mechanism:</p>
                        <p><div style="display: flex; justify-content: center;"><img src="../img/RNN-predict-detail.png" alt="alt text"></div></p>
                        <center>figure 4</center>
                        <p>Here is a very intuitive explanation. Imagine that RNN has seen some sentences like these:</p>
                        <ul>
                            <li>Today was sunny, I am very happy.</li>
                            <li>Today is sunny, I feel very happy</li>
                            <li>weather is good, I feel very happy</li>
                            <li>…</li>
                        </ul>
                        <p>Note that although some words are different, they have very similar meaning with others (e.g. <em>was</em> vs <em>is</em>), so with lots of these sentences going through RNN repeatedly, it <code>&quot;</code>learn<code>&quot;</code> to build an internal architecture which embody info about these sentences. Consequently, when you input</p>
                        <blockquote>
                            <p>Today is sunny, I _</p>
                        </blockquote>
                        <p>it would confidently predict the next word to be <em>am</em> or <em>feel</em>. When you input</p>
                        <blockquote>
                            <p>Today is sunny, I am very _</p>
                        </blockquote>
                        <p>it would confidently predict the next word to be <em>happy</em>.</p>
                        <p>The old FNN can not do this (as least not as easily as RNN), however, because it has no state information like RNN. Also, we can see that FNN’s input dimension is fixed, while RNN can scale to as many dimensions as needed. Moreover, RNN had been changed / extended to many other powerful model, like LSTM, GRU and word2vec.</p>
                        <p>Of course the application of RNN is not limited to word prediction. With a little change, we can achieve various usages. For example,</p>
                        <ul>
                            <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">char-rnn</a>, <em>character-level language models</em> by Andrej Karpathy</li>
                            <li>image/video captioning</li>
                            <li>translation</li>
                            <li>image processing</li>
                        </ul>
                        <h2>How to use RNN</h2>
                        <p>This is the main part of this essay. In this part, I will explain the mathematics behind RNN, and then give a implementation of it. Some knowledge of mathematics is required to go through this part, though (read carefully, it’s not that hard). You can also refer to <em>Razvan Pascanu et al.</em>, <a href="http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf">here</a> and <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">here</a> if having any difficulties going through the algorithms.</p>
                        <p><div style="display: flex; justify-content: center;"><img src="../img/rnn-mini.png" alt="rnn mini"></div></p>
                        <h4>The Forward Pass of RNN</h4>
                        <p>A generic recurrent neural network, with input <img src="../img/tex-img/t3RaeWY78ccHeJaT.svg" alt=" \inline \boldsymbol{u_{t}} " /> and state <img src="../img/tex-img/I9rpSmaPmTXZS9Oo.svg" alt=" \inline \boldsymbol{x_{t}} " /> (state <img src="../img/tex-img/pDDGjoBsdoMgJSwd.svg" alt=" \inline \boldsymbol{x_{t-1}} " /> is the previous state) for time step <img src="../img/tex-img/qKDsMnESUkXt5HOY.svg" alt=" t " />, is given by:</p>
                        <p><span style="float:right">(1)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/Va4mlaxKz3UUTD8s.svg" alt=" \boldsymbol{x_t} = F(\boldsymbol{x_{t-1}}, \boldsymbol{u_t}, \theta) " /></div></p>
                        <p><span style="float:right">(2)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/KDtRDnuzO8o4ouL1.svg" alt=" \boldsymbol{y_t} = \delta(\boldsymbol x_t)" /></div></p>
                        <p>Note that those bold symbols above are vectors. And eq.2 can be omitted if you want <img src="https://tex.s2cms.ru/svg/y_t" alt=" \inline \boldsymbol{y_{t}} " /> equal <img src="https://tex.s2cms.ru/svg/x_t" alt=" \inline \boldsymbol{x_{t}} " /></p>
                        <p>More generally:</p>
                        <p><span style="float:right">(3)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/OVvl6SXR3Ad0sDeU.svg" alt=" \boldsymbol{x_{t}} = \boldsymbol{W_{rec}}\sigma(\boldsymbol{x_{t-1}}) + \boldsymbol{W_{in}}\boldsymbol{u_{t}} + \boldsymbol{b}" /></div></p>
                        <p>In this case, the parameters of model are given by the recurrent weight matrix <img src="../img/tex-img/uTuwZGhGNNZMBtTk.svg" alt=" \inline \boldsymbol{W_{rec}}" />, the bias <img src="../img/tex-img/EZDaDCedQeRPZ7oI.svg" alt=" \boldsymbol{b} " /> and input weight matrix <img src="../img/tex-img/Gf7BNRy61iDELEhx.svg" alt=" \boldsymbol{W_{in}} " />, collected in <img src="../img/tex-img/Htyc9Dryk36giCf8.svg" alt=" \theta " /> for general case. <img src="../img/tex-img/wT98MANhqaAmM4c1.svg" alt=" x_{0} " /> is provided by the user, set to zero or learned, and <img src="../img/tex-img/xznB9K7Vyk7l0S2h.svg" alt=" \sigma " /> is an element-wise function such as <em>sigmoid</em>. A cost <img src="../img/tex-img/GMF0MRsaow9OatZT.svg" alt=" \boldsymbol\varepsilon = \sum_{1\leq t \leq T} \boldsymbol \varepsilon_t " /> messures the performance of the network of the given task. We define <img src="../img/tex-img/PHReE519APm5JFzN.svg" alt=" \boldsymbol\varepsilon_t = - y_j log(\hat{y_i}) - (1-y_j)log(1-\hat{y_j}) " />, which is the so-called <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a>. Here <img src="../img/tex-img/qTGRRNy7wvs9Wrmb.svg" alt=" y_j " /> is the correct answer(the label) and <img src="../img/tex-img/wWHTnqevmpBtHeI1.svg" alt=" \hat{y_j} " /> is the output of RNN.</p>
                        <hr>
<p style="font-size: 13px; margin: 0px 0px 5px 0px;">(ps. <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> is a very interesting function which is used in many other machine learning algorithm. I have write an essay about it <a href="./2017-01-12-thinking-cross-entropy.html">here</a>.</p>
                        <h4>The Backward Pass of RNN</h4>
                        <p>When training RNN, the most widely used algorithm is <strong>BPTT</strong> (BackPropagation Through Time). Essentially, BPTT is just like the ordinary BP algorithm used in Feedforward Neural Network. The thing that makes it distinguished from ordinary BP is that when backpropagating at node/time <img src="../img/tex-img/qKDsMnESUkXt5HOY.svg" alt=" t " />, BPTT generate the error gradient with respect to all nodes from <img src="../img/tex-img/GmSOA12LQhrmwGfY.svg" alt=" t-1 " /> to <img src="../img/tex-img/HASlkhdkrQ63G02R.svg" alt=" 1 " />, thus the name <strong>backpropagate through time</strong>.</p>
                        <p>The gradient is calculated as follow:</p>
                        <p><span style="float:right">(4)</span><div style="display: flex; justify-content: center;"><img src="../img/tex-img/S92HS60NZ7F0CE8X55KHI6LQ.svg" alt=" {\partial\boldsymbol\varepsilon \over \partial\theta} = 
\sum_{1 \leq t \leq T}
{\partial\boldsymbol\varepsilon_t \over \partial\theta}  " /></div></p>
                        <p><span style="float:right">(5)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/a9XFdnYliOSwjeRk.svg" alt=" {\partial\boldsymbol\varepsilon_t \over \partial\theta} = 
                            \sum_{1 \leq k \leq t}\big({\partial \boldsymbol\varepsilon_t \over \partial \boldsymbol x_t} {\partial \boldsymbol x_t \over \partial \boldsymbol x_k} {\partial^+\boldsymbol x_k \over \partial\theta} )" /></div></p>
                        <p><span style="float:right">(6)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/90f08SmdUSfaUYIt.svg" alt=" {\partial \boldsymbol x_t \over \partial \boldsymbol x_k} = 
                            {\prod_{k &lt; i \leq t} {\partial \boldsymbol x_i \over \partial \boldsymbol x_{i-1} } = 
                            {\prod_{k &lt; i \leq t} \boldsymbol W^T_{rec} {\mathit diag}(\sigma'(\boldsymbol x_{i-1}))" /></div></p>
                        <p>These equations were obtained by writing the grdients in a sum-of-product form[2].</p>
                        <p><img src="../img/tex-img/domOBEutcr2YzL00.svg" alt=" \inline \partial^+\boldsymbol x_k \over \partial\theta " /> refers to the <em>immediate</em> partial derivative of the state <img src="../img/tex-img/SxAkGSRbKBZYynUp.svg" alt=" \mathbf x_k " />
                        with respect to <img src="../img/tex-img/Htyc9Dryk36giCf8.svg" alt=" \theta " />, where <img src="../img/tex-img/1Z8HP4o2GJwlazNE.svg" alt=" \boldsymbol x_{k-1} " /> is taken as a constant with respect to <img src="../img/tex-img/Htyc9Dryk36giCf8.svg" alt=" \theta " />(more detail <a href="./archive/rnn-immediate-partial-derivative.html">here</a>). Specifically, considering eq.3, the value of any row <img src="../img/tex-img/iLbqQ8I1gSzJvLqc.svg" alt=" i " /> of the matrix <img src="../img/tex-img/SwnwC72koGbZwn2D.svg" alt=" \partial^+\boldsymbol x_k \over \partial \boldsymbol W_{rec}" /> is just <img src="../img/tex-img/4uNC5iDz6Skj22nP.svg" alt=" \sigma(\boldsymbol x_{k-1})" />. Eq.6 also provides the form of Jacobian matrix <img src="../img/tex-img/j0umxUkV8YKN2tYF.svg" alt=" \partial \boldsymbol x_i \over \partial \boldsymbol x_{i-1} " /> for the specific parametrization given in eq.3, where <img src="../img/tex-img/hJwpD6KkmjS3IP6h.svg" alt=" \mathit diag " /> convert a vector into a diagnol matrix, and <img src="../img/tex-img/EHRtBLBffMraDNUn.svg" alt=" \sigma' " /> compute the element-wise derivative of <img src="../img/tex-img/0QWQ7OJZidiXYQvS.svg" alt="\sigma" />.</p>
                        <p>Any gradient component <img src="../img/tex-img/z6ixM0SDjZ4Y3Uyp.svg" alt=" \partial \boldsymbol\varepsilon_t \over \theta " /> is also a sum(see eq.5), whose terms we refer to as <em>temporal contribution</em> or <em>temporal component</em>. One can see that each such temporal contribution <img src="../img/tex-img/3K30pahQsvPrw6Z6.svg" alt=" {\partial \boldsymbol\varepsilon_t \over \partial \boldsymbol x_t} {\partial \boldsymbol x_t \over \partial \boldsymbol x_k} {\partial^+\boldsymbol x_k \over \partial\theta} " /> measures how <img src="../img/tex-img/Htyc9Dryk36giCf8.svg" alt=" \theta " /> at step <img src="../img/tex-img/iDZKc2k6VHqH7CxH.svg" alt=" k " /> affects the cost at step <img src="../img/tex-img/j21GUaHfx7GDzmGJ.svg" alt=" t &gt; k " /> and the factors <img src="../img/tex-img/P4wh3ViwM1aFfrdv.svg" alt=" \partial \boldsymbol x_t \over \partial \boldsymbol x_k " /> (eq.6) transport the error <em>in time</em> from step <img src="../img/tex-img/qKDsMnESUkXt5HOY.svg" alt=" t " /> back to step <img src="../img/tex-img/iDZKc2k6VHqH7CxH.svg" alt=" k " /> (because <img src="../img/tex-img/6rlJB6m0jujTp1g4.svg" alt="{\partial \boldsymbol x_t \over \partial \boldsymbol x_k} =  {\partial \boldsymbol x_t \over \partial \boldsymbol x_{t-1}}  {\partial \boldsymbol x_{t-1} \over \partial \boldsymbol x_{t-2}}... {\partial \boldsymbol x_{k+1} \over \partial \boldsymbol x_k} " />). This is the essence of <strong>Backpropogationo Through Time</strong>. This way, the error is back-propogated from time <img src="../img/tex-img/qKDsMnESUkXt5HOY.svg" alt=" t " /> to time <img src="../img/tex-img/iDZKc2k6VHqH7CxH.svg" alt=" k " />, <img src="../img/tex-img/MAeH8EJKvF6mEENo.svg" alt=" k = 1,2,..,t " />.</p>
                        <p>When I implement a example RNN below I would give a concrete derivation of these gradient so that you can see it clearly.</p>
                        <hr>
                        <p style="font-size: 13px; margin: 0px 0px 5px 0px;">One more thing: we would further loosely distinguish between <em>long term</em> and <em>short term</em> contributions, where long term refers to components for which <img src="../img/tex-img/4X6gJfUS7N2YtXkw.svg" alt=" k \ll t " /> and short term to everything else.</p>
                        <h4>Exploding and Vanishing Gradient</h4>
                        <p>When using BPTT to train RNN, one can easily get trapped into the <em>exploding gradient problem</em> or <em>vanishing gradient problem</em>. It turn out that 2-norm, which you can think of as absolute value, of the above Jacobian matrix can grow to extremely large (exploding) or small (vanishing) when <img src="../img/tex-img/4X6gJfUS7N2YtXkw.svg" alt=" k \ll t " />. This problem was first introduced in Bengio et al. (1994). In that paper, the <img src="../img/tex-img/oLv2tqFmyuB4LJ0r.svg" alt=" exploding " /> <img src="../img/tex-img/2gxiyWWdFubBYkXF.svg" alt=" gradients " /> problem refers to the large increase in the norm of the gradient during training. Such events are due to the explosion of the long term components, which can grow exponentially more than short term ones. The <img src="../img/tex-img/ERppo3aYOFjk15M8.svg" alt=" vanishing " /> <img src="../img/tex-img/2gxiyWWdFubBYkXF.svg" alt=" gradients " /> problem refers to the opposite behaviour, when long term components go exponentially fast to norm 0, making it impossible for the model to learn correlation between temporally distant events.</p>
                        <p>We can easily see the problem from eq.6. When we calculate a long-term contribution, i.e. <img src="../img/tex-img/4X6gJfUS7N2YtXkw.svg" alt=" k \ll t " />, eq.6 would explode or vanish (Multiplying a number <img src="../img/tex-img/wQoVhoeHTOo17Wuf.svg" alt=" x &gt; 1 " /> multiple times would cause the product to explode eventually, and vanish if <img src="../img/tex-img/TdyXevDoGWPr8qCD.svg" alt=" x &lt; 1 " />)</p>
                        <p>Here is a example regarding the <em>vanishing gradient problem</em>:</p>
                        <p>sentence-1:</p>
                        <blockquote>
                            <p><em>Jane walked into the room. John walked in too. Jane said hi to __</em></p>
                        </blockquote>
                        <p>sentence-2:</p>
                        <blockquote>
                            <p><em>Jane walked into the room. John walked in too. It was late in the day, and everyone was walking home after a long day at work. Jane said hi to __</em></p>
                        </blockquote>
                        <p>Ideally, RNN should predict both to be <code>&quot;</code>John<code>&quot;</code> . However, in practice, RNN may fail at sentence-2. This is because during the back-propapation phase, the contribution of gradient values gradually vanishes as they propagate to earlier time steps. Thus, for lone sentences, the probability that <code>&quot;</code>John<code>&quot;</code> would be recognized as the next word reduces with the size of the context.</p>
                        <h4>Solutions to the Exploding &amp; Vanishing Gradients problem</h4>
                        <p>Researcher have propose many solution to solve the Exploding &amp; Vanishing problem. Below are some famous ones.</p>
                        <ul>
                            <li>
                                <p><a href="./2017-01-14-thinking-LSTM.html">LSTM</a></p>
                            </li>
                            <li>
                                <p><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a></p>
                            </li>
                            <li>
                                <p><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">GRU</a></p>
                            </li>
                        </ul>
                        <h2>Difference between RNN and FNN</h2>
                        <p>Below are two gif that can be used to show the different between RNN and FNN. These two gif are from <a href="https://zhuanlan.zhihu.com/p/24720659">Yjango’s website</a>:</p>
                        <p align="center"><img align="center" src="../img/RNNunfold.gif" alt="RNN unfolded"></p>
                        <center>RNN unfolded</center>
                        <p align="center"><img align="center" src="../img/FNNunfold.gif" alt="FNN unfolded"></p>
                        <center>FNN unfolded</center>
                        <p>You may have noticed the core difference: there are connection between sibling nodes in RNN, while not in FNN.</p>
                        <h2>Implementation of RNN in python</h2>
                        <p>Now we implement a simple RNN in python. In this example implementation, we use plain numbers instead of words as input, since using words as input would requires that we transform every word to a <em>word vector</em> for RNN to train, which is another topic and is out of the scope of this post. We leave it to another essays.</p>
                        <p>In this simple example, we want to do is build a RNN, train it on a sequence of numbers, hopefully it can figure out the pattern in that numbers sequence and the predict the next one. For simplicity, here we assume that all the numbers are bound within [0, 1].</p>
                        <hr>
                        After training, the RNN would behave as follow:
                        <pre>
                        <p>Pass 1</p>
         input:   0.1
         predict: 0.998
                        <p>next pass</p>
         input:    0.1,   0.2
         predict: 0.998, 0.189
                        <p>. . . </p>
                        <p>next pass</p>
         input:   0.1,   0.2,   0.3,   0.4,   0.5,   0.6,   0.7,   0.9
         predict: 0.998, 0.189, 0.399, 0.478, 0.601, 0.702, 0.788, 0.870
                        </pre>
                        <hr>
                        <h3>Python code</h3>
                        <!-- see `https://github.com/google/code-prettify` for more detail -->
                        <?prettify lang=c linenums=false?>
                        <pre class="prettyprint">
#!/usr/bin/python3
#coding:utf-8

# pylint: disable=superfluous-parens, invalid-name, broad-except, missing-docstring

import math

#initialize weight
w = 0.5
v = 0.5
b = 0.5
learning_rate = 0.5

#    x_t_sum = w * x_t + v * u_t + b
#    y_t_sum = w * x_t + v * u_t + b
#    x_t = sigmoid(x_t_sum),
#    y_t = sigmoid(y_t_sum),
#(we could change any sigmoid to tanh)
#where:
#w is the recurrent weight,
#v is the input weight
#b it the bais

def sigmoid(x):
    return 1 / (1 + math.exp(-x))

def sum_recursive(seq, pred_seq, upperbound, r, previous_part):
    if r &lt; 0:
        return 0
    if r == 0:
        x_t_own = 0
    else:
        x_t_own = pred_seq[r-1]
    in_t = seq[r]
    sg = sigmoid(w * x_t_own + v * in_t + b)
    own_part = sg * (1 - sg) * x_t_own

    if r == upperbound:
        left_part = 1
    else:
        left_own = pred_seq[r]
        left_in_t = seq[r+1]
        sg_left_own = sigmoid(w * left_own + v * left_in_t + b)
        left_part = sg_left_own * (1 - sg_left_own) * w
        left_part = previous_part * left_part

    return (left_part * own_part) + sum_recursive(seq, pred_seq, upperbound, r-1, left_part)

def calculate_w_grad(seq, pred_seq, i):
    # + 0.0001 so that it won't cause Divided-by-zero-Exception
    de_over_dxt = -seq[i] / (pred_seq[i] + 0.0001) + (1 - seq[i]) / (1 - pred_seq[i])
    g = sum_recursive(seq, pred_seq, i, i, 1)
    return de_over_dxt * g

def calculate_v_grad(seq, pred_seq, i):
    # + 0.0001 so that it won't cause Divided-by-zero-Exception
    de_over_dxt = -seq[i] / (pred_seq[i] + 0.0001) + (1 - seq[i]) / (1 - pred_seq[i])
    if i == 0:
        x_t_1 = 0
    else:
        x_t_1 = pred_seq[i-1]
    in_t = seq[i]
    sg = sigmoid(w * x_t_1 + v * in_t + b)
    derivative = sg * (1 - sg)
    return de_over_dxt * derivative * in_t

def calculate_b_grad(seq, pred_seq, i):
    # + 0.0001 so that it won't cause Divided-by-zero-Exception
    de_over_dxt = -seq[i] / (pred_seq[i] + 0.0001) + (1 - seq[i]) / (1 - pred_seq[i])
    if i == 0:
        x_t_1 = 0
    else:
        x_t_1 = pred_seq[i-1]
    in_t = seq[i]
    sg = sigmoid(w * x_t_1 + v * in_t + b)
    derivative = sg * (1 - sg)
    return de_over_dxt * derivative

def rnn(in_seq):
    global w
    global v
    global b

    # train RNN with 3000 round
    for _ in range(3000):
        x_t_1 = 0
        pred_seq = []
        for index, in_t in enumerate(in_seq):
            x_t = sigmoid(w * x_t_1 + v * in_t + b)
            pred_seq.append(x_t)

            w_gradient = 0
            v_gradient = 0
            b_gradient = 0

            for i in range(index + 1):
                w_gradient += calculate_w_grad(in_seq, pred_seq, i)
                v_gradient += calculate_v_grad(in_seq, pred_seq, i)
                b_gradient += calculate_b_grad(in_seq, pred_seq, i)

            w_gradient = w_gradient / (index + 1)
            v_gradient = v_gradient / (index + 1)
            b_gradient = b_gradient / (index + 1)

            w = w - learning_rate * w_gradient
            v = v - learning_rate * v_gradient
            b = b - learning_rate * b_gradient

            x_t_1 = x_t

    print(&quot;Finished training: w: %f, v: %f, b: %f&quot; % (w, v, b))

    print(&quot;Start to predict:&quot;)
    x_t_1 = 0
    for in_t in in_seq:
        x_t = sigmoid(w * x_t_1 + v * in_t + b)
        print(&quot;Result %f\t%f&quot; % (in_t, x_t))
        x_t_1 = x_t

rnn([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])
                        </pre>

                        <p>Try run this snippet youself.</p>
                        <hr>
                        <p>To help you understand the code above, let me illustrate the basic structure of the rnn above and calculate the gradients(derivatives) for you.</p>
                        <p>The basic structure is pretty much the same as that in <em>figure 1</em> above:</p>
                        <p><div style="display: flex; justify-content: center;"><img src="../img/python-rnn-basic-arch.png" alt="alt-text"></div></p>
                        <center>figure 5</center>
                        <p>The unrolled version would be:</p>
                        <p><div style="display: flex; justify-content: center;"><img src="../img/python-rnn-arch-unroll.png" alt="alt-text" style="width: 700px; height: 400px;"></div></p>
                        <center>figure 6</center>
                        <p>This RNN predict the output given the current input, previous state(previous output) and current weights.</p>
                        <p>We calculate the error gradient as follow.</p>
                        <p>We denote the cost function as:</p>
                        <p><span style="float:right">(7)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/UvWJtPlfgi4604Zq.svg" alt=" J = - y_t log(\hat{y_t}) - (1-y_t)(1-log(\hat{y_t})) " /></div></p>
                        <p>where <img src="../img/tex-img/C1k0xEhthqJ0krNm.svg" alt="\inline \hat{y_t}" /> is the actual output and is calculated as:</p>
                        <p><span style="float:right">(8)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/urqioJ2cRbqFp90F.svg" alt=" \hat{y_t} = \sigma(w x_{t-1} + v in_t + b) " /></div></p>
                        <p>Here we use the same function for <img src="../img/tex-img/50KM1go8pV7aWGF1.svg" alt=" x_t " />:</p>
                        <p><span style="float:right">(9)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/eNfBN0Vx7DYvY7E8.svg" alt=" x_t = \sigma(w x_{t-1} + v in_t + b) " /></div></p>
                        <p>In the above two formulas:</p>
                        <p><span style="float:right">(10)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/FcjpyhyPkURUMdOr.svg" alt=" \sigma(x) = {1 \over {1 + e^{-x}}} " /></div></p>
                        <p>This way, the gradients are:</p>
                        <ul>
                            <li>The error gradient for <img src="../img/tex-img/9tCun4SCZnVM02wW.svg" alt="w" /> is:</li>
                        </ul>
                        <p><span style="float:right">(11)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/ufuoM6o4dMm5zpSA.svg" alt=" {dJ \over dw} = 
                        \sum_{1 \leq k \leq t}
                        ({\partial J \over \partial x_t}
                        {\partial x_t \over \partial x_k}
                        {\partial ^+ x_k \over \partial w}) =
                        {\partial J \over \partial x_k} 
                        \sum_{1 \leq k \leq t}(
                        {\partial x_t \over \partial x_k}
                        {\partial ^+ x_k \over \partial w}) " /></div></p>
                        <ul>
                            <li>error gradient for <img src="../img/tex-img/4iUYx7uMMSb5Pdia.svg" alt="v" /> is</li>
                        </ul>
                        <p><span style="float:right">(12)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/cBhd756R2BP3cotY.svg" alt=" {dJ \over dv} = 
                        \sum_{1 \leq k \leq t}
                        ({\partial J \over \partial in_t}
                        {\partial in_t \over \partial in_k}
                        {\partial ^+ in_k \over \partial v}) = 
                        {\partial J \over \partial in_t} \sum_{1 \leq k \leq t} (
                        {\partial in_t \over \partial in_k}
                        {\partial ^+ in_k \over \partial v})" /></div></p>
                        <p>because there is no relation between <img src="../img/tex-img/XF4NrHOHKykOUhMR.svg" alt="in_t" /> and <img src="../img/tex-img/fFtj7qN1TKBEQ5uk.svg" alt="in_k" />, where <img src="../img/tex-img/HyepHLLJboA3oA2M.svg" alt="k = 1,2,..,k-1" />, we <strong>don’t</strong> have to write this derivative as <em>immediate</em> partial derivative form:</p>
                        <p><span style="float:right">(13)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/AFg05rUtk9arFTXD.svg" alt=" {dJ \over dv} = {dJ \over d\hat{y}} {d\hat{y} \over dv} = 
                        ({-y_t \over \hat{y_t}} + {{1-y_t} \over {1-\hat{y_t}}})
                        \sigma(w x_{t-1} + v in_t + b)(1 - \sigma(w x_{t-1} + v in_t + b)) in_t " /></div></p>
                        <ul>
                            <li>error gradient for <img src="../img/tex-img/5ePYdbhaRCT3xYrL.svg" alt="b" /> is</li>
                        </ul>
                        <p><span style="float:right">(14)</span><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/FihWB3zo3Y1lueZQ.svg" alt=" {dJ \over db} = {dJ \over d\hat{y}} {d\hat{y} \over db} = 
                        ({-y_t \over \hat{y_t}} + {{1-y_t} \over {1-\hat{y_t}}})
                        \sigma(w x_{t-1} + v in_t + b)(1 - \sigma(w x_{t-1} + v in_t + b))" /></div></p>
                        <p>And then we subtract the gradients from the previous parameter:</p>
                        <p><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/jynjksaFMZ3r6fL4.svg" alt=" w := w - \gamma w " /></div></p>
                        <p><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/sGKyLYedRN0b5QKU.svg" alt=" v := v - \gamma v " /></div></p>
                        <p><div style="display: flex; justify-content: center;"><img  src="../img/tex-img/7N77dsPoK2X7BeMv.svg" alt=" b := b - \gamma b " /></div></p>
                        <p>where <img src="../img/tex-img/9LbQpi5pLPVOZttk.svg" alt=" \gamma " /> is the learning rate.</p>
                        <h2>References</h2>
                        <ol>
                            <li>
                                <p>Bengio et al. (1994), <em>Learning long-term dependency with gradient descent is difficult</em>, <em>IEEE Transactions on Neural Networks</em>, 5(2), 157-166</p>
                            </li>
                            <li>
                                <p>Razvan Pascanu, Tomas Mikolov, Yoshua Bengio, <em>On the difficulty of training recurrent neural networks</em>, JMLR W&amp;CP 28 (3) : 1310–1318, 2013</p>
                            </li>
                        </ol>
                        <hr>
                        <p class="LastModified">Yubin Ruan, last modified on 2016-12-12</p>
                    </div>  <!-- end blog content -->

                    <!--  here should be some pictures for seperating -->
                    <div class="Comments" > <!-- start comments content -->
                        <p><span style="font-size: 1.5em; font-weight: bold;">Comments</span> <span style="font-size: 0.7em">powered by disqus</span></p>

                        <div id="disqus_thread"></div>
                        <script>

var disqus_config = function () {
    this.page.url = 'http://walkerlala.github.io/archive/2016-12-12-rnn-explain-impl.html'; 
    this.page.identifier = '2016-12-12-rnn-explain-impl'; 
};
(function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//walkerlala-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
                        </script>
                        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                    </div> <!-- end comments content -->

                    <hr>
                    <div id="footer">
                        Copyright © 2015-2018 Yubin Ruan
                        <br>
                    </div>

                </div> <!--main-->
                <p>  &nbsp;   </p>
            </div> <!--wrap-->
        </div> <!--wrapper-->
    </body>
</html>
