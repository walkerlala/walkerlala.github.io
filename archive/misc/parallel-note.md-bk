- **live lock** , Too busy to respond to others to resume work. They are not blocking. *e.g.*, two people meet in corridor:  1) A left B left -> A right B right -> A left B left .... infinite loop

- **dead lock**, 

- Philosopher dinning problem solutions

  - Resource Hierarchy
  - Semaphores / Arbistrator
  - Chandy / Misra (use dirty / clean fork to indicate priority )

- priority inversion solution (assume priority of **H**, **M**, **L**)

  * locking with interrupts disabled

    So that **M** cannot pre-empt **L** because pre-emption require interrupt

  * priority ceiling

    the mutex give his *high* priority to **L**

  * priority inheritance

    **L** inherits **H**'s priority so that **M** cannot pre-empt 

  * Random boosting

    ready task holding lock are randomly boosted in priority until they exit the critical region　（used by **Microsoft Windows**$$^{TM}$$)

  * Avoid blocking by using **Non-blocking synchronization** or **Read-copy-update** 

- producer-consumer problem

  - bonus: why this snippet is wrong?

    ```c
    int itemCount = 0;

    procedure producer() {
        while (true) {
            item = produceItem();

            if (itemCount == BUFFER_SIZE) {
                sleep();               // <- sleep with nothing waken up
            }

            putItemIntoBuffer(item);
            itemCount = itemCount + 1;

            if (itemCount == 1) {
                wakeup(consumer);      // <- can't wake up before haven't sleep
            }
        }
    }

    procedure consumer() {
        while (true) {

            if (itemCount == 0) {
                sleep();            // <-- preempted before sleep
            }

            item = removeItemFromBuffer();
            itemCount = itemCount - 1;

            if (itemCount == BUFFER_SIZE - 1) {
                wakeup(producer);
            }

            consumeItem(item);
        }
    }
    ```

    this would cause dead-lock

  - A refined one using semaphore:

    ```c
    semaphore fillCount = 0; // items produced
    semaphore emptyCount = BUFFER_SIZE; // remaining space

    procedure producer() {
        while (true) {
            item = produceItem();
            down(emptyCount);
            putItemIntoBuffer(item);
            up(fillCount);
        }
    }

    procedure consumer() {
        while (true) {
            down(fillCount);
            item = removeItemFromBuffer();
            up(emptyCount);
            consumeItem(item);
        }
    }
    ```

    This work fine with only on producer and consumer.(when there are more than one producer, they would possibly write to the same slot at the same time )

  - final solution using semaphor with a mutex

    ```c
    mutex buffer_mutex; // similar to "semaphore buffer_mutex = 1", but different (see notes below)
    semaphore fillCount = 0;
    semaphore emptyCount = BUFFER_SIZE;

    procedure producer() {
        while (true) {
            item = produceItem();
            down(emptyCount);
                down(buffer_mutex);    // <- lock the buffer first
                    putItemIntoBuffer(item);
                up(buffer_mutex);
            up(fillCount);
        }
    }

    procedure consumer() {
        while (true) {
            down(fillCount);
                down(buffer_mutex);
                    item = removeItemFromBuffer();
                up(buffer_mutex);
            up(emptyCount);
            consumeItem(item);
        }
    }
    ```

- Reader-writer problem

  - The first readers-writers problem:  The writer may starve (*reader preference*)
  - The second readers-writers problem: The reader may starve (*writer preference*)
  - The third readers-writers problem:  With a FIFO semaphores, neither readers nor writers will starve

- seqlock -- a improved version of reader-writer-lock(rwlock)

  - writer increment sequence number after acquiring lock and before releasing lock

  - readers check the sequence number before and after reading the shared data

    (If the sequence number is odd on either occasion, a writer had taken the lock while the data was being read and it **may or may not** have changed. If the sequence numbers are different, a writer has changed the data while it was being read. In either case readers simply retry (using a loop, or in Linux, `read_seqretry()`) until they read the same even sequence number before and after)

  *Features*:

  - In Linux, it use spin lock to implement
  - readers would not be blocked
  - suitable for situation where write is rare but read is frequent

  *References*

  - https://en.wikipedia.org/wiki/Seqlock
  - https://lwn.net/Articles/22818/

- RCU

  **read-copy-update** (**RCU**) is a synchronization mechanism implementing a kind of mutual exclusion that can sometimes be used as an alternative to a [readers-writer lock](https://en.wikipedia.org/wiki/Readers-writer_lock). It allows extremely low overhead, [wait-free](https://en.wikipedia.org/wiki/Non-blocking_synchronization) reads(allow read to occur concurrently with update). However, RCU updates can be expensive, as they must leave the old versions of the data structure in place to accommodate pre-existing readers. These old versions are reclaimed after all pre-existing readers finish their accesses.

  In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads **regardless of whether they be readers or updaters**, or with reader-writer locks that **allow concurrent reads but not in the presence of updates**, **RCU supports concurrency between a single updater and multiple readers**. RCU ensures that reads are coherent by maintaining multiple versions of objects and ensuring that they are not freed up until all pre-existing read-side critical sections complete. RCU defines and uses efficient and scalable mechanisms for publishing and reading new versions of an object, and also for deferring the collection of old versions. These mechanisms distribute the work among read and update paths in such a way as to make read paths extremely fast. In some cases (non-preemptable kernels), RCU's read-side primitives have zero overhead.

  At its core, RCU is nothing more or less than an API that provide:

  1. a publish-subscribe mechanism for adding new data (`rcu_assign_pointer()`, `rcu_dereference()`)
  2. a way of waiting for pre-existing RCU readers to finish, (*e.g,* `synchronize_rcu()`)
  3. a discipline of maintaining multiple version to permit change without harming or unduly delay concurrent RCU readers

  $$\blacksquare$$ **Advantage**

  - extremely low overhead of read
  - Immunity to read-side deadlock (because read-size do not involve any spin, block, and backwark branches )
  - Immunity to priority inversion (because low-priority RCU readers cannot prevent a high-priority RCU updater from acquiring the update-side lock. Similarly, a low-priority RCU updater cannot prevent high-priority RCU readers from entering an RCU read-side critical section)
  - small real-time latency
  - RCU readers and updaters run concurrently

  $$\blacksquare$$  **reference**

  [1] : https://lwn.net/Articles/262464/

  [2] : https://lwn.net/Articles/263130/

  [3] : https://lwn.net/Articles/264090/

  (there is no any formal memory model yet to define this precisely. See McKenny's note)
  Regarding to memory barriers, there are some confusions about **See by All the CPU** and **See by only the current CPU** , for `smp_mb()`, all CPUs will regard the ordering(maybe except Alpha). But with `smp_wmb()` and `smp_rmb()`, things are pretty different. See the discussion with me and Paul Mckenny

- Memory barier transitivity

  - Although memory barier only guarantee to have effect on **current** CPU, all mainstream CPU provide a feature called **memory barier transitivity**: 

    > If **B** saw the effects of **A**'s accesses, and **C** saw the effects of **B**'s accesses, then **C** must also see the effects of **A**'s accesses.

    All CPUs I am aware of claim to provide transitivity.

- A good elaboration of memory ordering on modern processors [by Paul McKenney](./mem-ordering-Paul-McKenney.pdf)

- Memory barrier only enforce ordering among multiple memory references: They do absolutely nothing to expedite the propogation of data from one part of the system to another. This leads to **a quick rule of thumb**:  You do not need memory barriers unless you are using more than one variable to communicate between multiple threads.

- The GCC extension`__sync_synchronize()` primitive issues a "memory barrier", which constrains **both the compiler's and the CPU's ability** to reorder operations. The `barrier()`in Linux kernel only constrain compiler's ability to reorder operation. `READ_ONCE()` prevents compilers from optimizing away a given memory read, and `WRITE_ONCE()` prevents them from optimizing away a given memory write:

  ```c
  /* turn it into a volatile and then read it */
  #define ACCESS_ONCE(x)  (* (volatile typeof(x) *) &(x))

  #define READ_ONCE(x)   ACCESS_ONCE(x)
  #define WRITE_ONCE(x, val) ({ ACCESS_ONCE(x) = (val); })
  #define barrier()    __asm__ __volatile__("": : :"memory")
  ```

- In Linux kernel, normal **non-tearing reads** and **stores** are provided by `atomic_read()` and `atomic_write()`. **Acquire load** is provided by `smp_load_acquire()` and **release store** by `smp_store_release()`.  What is these *non-tearing* read/store and *acquire* load/store? TODO

  Note\*, in Linux, many atomic operation would only preserve its atomic semantics as long as other accesses of the atomic variable are performed through atomic_xxx operations(e.g, the atomic operation interface). How can this be guaranteed ?

- Quick Quiz 5.17

- **Partial Partitioning**:  partitioning applied only to common code path (think *parallel fastpath*)


--------

A pragmatic definition of the word "contention":

> A key issue with the performance of concurrent data structures is the level of memory **contention**: **the overhead in traffic to and from memory** as a result of **multiple threads concurrently attempting to access the same locations in memory**. This issue is most acute with blocking implementations in which locks control access to memory. In order to acquire a lock, a thread must repeatedly attempt to modify that location. On a [cache-coherent](https://en.wikipedia.org/wiki/Cache_coherence) multiprocessor (one in which processors have local caches that are updated by hardware in order to keep them consistent with the latest values stored) this results in long waiting times for each attempt to modify the location, and is exacerbated by the additional memory traffic associated with unsuccessful attempts to acquire the lock.

From wikipedia, [concurrent data structure](https://en.wikipedia.org/wiki/Concurrent_data_structure#Basic_principles)

-----

- **Sequential consistency** requires that the operations should appear to take effect in the order they are specified **in each program**. Basically it enforces **program order** within each individual process but NOT the whole system $$\blacksquare$$

- **Quiescent consistency**: An execution of a concurrent program is *quiescently consistent* if the method calls can be correctly arranged retaining the mutual order of calls separated by *quiescence*, **a period of time where no method is being called in any thread**. It requires **non-overlapping** operations to appear to take effect in their **real-time order**, but **overlapping operations might be reordered** $$\blacksquare$$

  Let's use a example to illustrate the differences between sequential consistency and quiescent consistency:

  ```
  P1 -- q.enq(x) -----------------------------------
  P2 ---------------- q.enq(y) ---- q.deq() --------
  ```

  If the system is quiescent consistent, the last `q.deq()` will guarantee to produce x. If the system is sequential consistent, the last `q.deq()` might may produce y (a.k.a program order)

  For a good example why sequential consistency is NOT compositional, see [here](https://cs.stackexchange.com/questions/54748/why-is-quiescent-consistency-compositional-but-sequential-consistency-is-not). For a good example why quiescent consistency is compositional, see [here]()

  **THE QUESTION IS: how to define those quiescent point???**

- **Linearizability**: 

  Each method call should appear to take effect instantaneously at some moment between its invocation and response.  see [this talk outline](http://fpl.cs.depaul.edu/jriely/papers/2014-qqc-talk.pdf)



​	You definitely want to read [this talk outline](http://fpl.cs.depaul.edu/jriely/papers/2014-qqc-talk.pdf) for more info on *memory consistency model*