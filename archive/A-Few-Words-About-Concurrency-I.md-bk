## ![alt text](../img/icons/png/microphone-1.png) A Few Words About Concurrency (part I)

Let's talk about concurrency.

Concurrency has always been a big topic ever since the 1960s. I learned about this topic from my Operating System courses one year ago but never feel confident to tell any other people that I really understand it<sup>[1]</sup>. I do have some insights into this topic and have read a few books/articles related to it. However, the more I read and know about it, the more it seems to be left unknown. I suppose this is probably the reason for the old saying "*the consequence of knowing too much*".

Nevertheless, I want to summarize and present some "cases" that I find really interesting these days as I read and think. I will try to show you how we can  get into the "dark" side of those techniques, with some necessary theories<sup>[1]</sup>. 

Specifically, in this essay, I would try show you how to implement a lock in the kernel, and what techniques and theories are required.

### \#1, Common Scenario of Concurrency

Ask yourself what is concurrency.

Probably you would come up with cases where programs do things in parallel. Good. For simplicity, let's summarize them into two cases:

1. programs running in parallel do their own business and do not interfere with each other. It is as if they are running in several different computers
2. programs running in parallel have to cooperate together to finish some tasks, where they have to **share memory**, **share disk**, **share network** ... etc. In a word, they all have something to share with others and want something from others.

Let's focus on the second case.

Imagine two programs are all updating the same variables, as follow:

```c
//global variable accessable by process 1 and 2
int balance = 0;

//process 1                         //process 2
for(int i=0;i<100000;i++) {         for(int k=0; k<100000; k++)
      ++blance;                         --blance;
}                                   }
```

what value would `blance` have when these loops are all done?  0 ? Hmm ... probably not. Go implement it yourself. Here is one of my experiments using C on Linux/pthread: ![alt tex](../img/icons/svg/file-code.svg) [increment-balance.c](../code/increment-balance.c) You can try it, compile it and see what the output is.

Why would you get result other than 0? Because assignments like `++balance` and `--balance` is not *atomic*. For example, they can be compiled to assembly like this:

```assembly
;;;; ++balance
mov 0x8048780, %eax    ;; 1
add $0x1, %eax         ;; 2
mov %eax, 0x8049780    ;; 3

;;;; --balance
mov 0x8049780, %eax    ;; 4
sub $0x1, %eax         ;; 5
mov %eax, 0x8049780    ;; 6
```

In this occasion, if the sequence of execution is **1 -> 4 -> 2 -> 5 -> 3 -> 6**, the effect of `add` would "cancel" that of `sub` (you can see **only** an `add` rather than an `add` with a `sub`)

Now you get the point: **processes accessing the same resources simultaneously without synchronization or mutual exclusion techniques would produce wrong results inevitably**

To make the program above correct, we can use *atomic* operation like this: 

```c
//global variable accessable by process 1 and 2
int balance = 0;

//process 1                                  //process 2
for(int i=0;i<100000;i++) {                  for(int k=0; k<100000; k++) {
  //atomically add one to `balance'          //atomically substract one from `balance'
  __sync_fetch_and_add(&balance, 1);            __sync_fetch_and_sub(&balance, 1);
}                                            }
```

You can test it using the program I wrote before: ![alt tex](../img/icons/svg/file-code.svg) [atomic-increment-balance.c](../code/atomic-increment-balance.c).

Note that the `__sync_fetch_and_add()` and `__sync_fetch_and_sub()` are primitive atomic operations built into the GCC compiler. You can use some other standard functions specified by C99 or C++11, such as `atomic_fetch_add()` and `atomic_fetch_sub()`. In Java, there are atomic integer types like `AtomicInteger` that have these atomic operations built in already.

What if we cannot use atomic operations? For example, imagine that we do not want to do `++balance` or `--balance`, instead we are writing to a file using two threads. We want the two threads write to the file piece by piece, that is, thread 1 have to wait for thread 0 to complete its write before it can continue and vice versa, otherwise contents from two threads would mix together. Obviously you don't have atomic operations for writing files. In this situation, we have to use some other mechanism, like a *lock* .

With locks, we can rewrite the program this way:

```c
//global variable accessable by process 1 and 2
int balance = 0;

//initialize a global lock accessable by process 1 and 2
pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;

//process 1                              //process 2             
for(int i=0; i<100000; i++) {            for(int k=0; k<100000; k++){
  pthread_mutex_lock(&mutex);              pthread_mutex_lock(&mutex);
  ++balance;                               --balance;
  pthread_mutex_unlock(&mutex);            pthread_mutex_unlock(&mutex);
}                                        }
```

You can test it using the program I wrote before: : ![alt tex](../img/icons/svg/file-code.svg) [lock-increment-balance.c](../code/lock-increment-balance.c).

In this example, we are using a lock, which is called *mutex* in the pthread world, to protect the variable `balance`. 

For writing files, locks can also be used:

```c
//process 1                              //process 2             
for(int i=0; i<100000; i++) {            for(int k=0; k<100000; k++){
  pthread_mutex_lock(&mutex);              pthread_mutex_lock(&mutex);
  writing_to_file();                       writing_to_file();
  pthread_mutex_unlock(&mutex);            pthread_mutex_unlock(&mutex);
}                                        }
```

### \# 2, Formalization and Terminologies

We have covered some basic things so far. Now let's formalize it before we can go further.

#### Critical Section

By definition, the part of program where shared memory is accessed is called **critical section** (or *critical region*). In our examples above, the critical section is the parts where we access the global variable *balance* (`++balance` and `--balance`) and write to file (`writing_to_file()`).

#### Race Condition

If more than one processes are allowed to stay in the critical section at the same time, the program is subjected to **race condition**, where unexpected things would happen. So, usually when we write concurrent codes, we want to avoid race condition so that our programs can behave correctly. For example, in the above example, the variable `balance` will eventually become `0` if we use atomic operation or locks.

### \# 3, How to Avoid Race Condition

As stated above, there are lots of methods for avoiding race condition. In this essay, we would try to use *lock* to achieve that. Before we go deeper to the detail implementation, let's be careful and define all the properties that we want for our lock.

#### Criteria for Avoiding Race Condition

There are the criteria that **have to** be met when using some methods to avoid race condition.

- Safety (a.k.a *mutual exclusion*)
  - No more than one process/thread in critical section at a time
- Liveness (a.k.a *progress*)
  - If multiple threads simultaneously request to enter critical section, **at least one** of them must be allowed to enter the critical section.
  - We must not depend on threads outside the critical section to make decision
- Bounded waiting (a.k.a *starvation-free*)
  - We must eventually allow waiting thread to proceed. In other word, every thread should have a chance to enter the critical section
- Make no assumptions about the speed and number of CPU

#### Desirable Properties for Avoiding Race Condition

Besides those criteria, it would be better if the mechanisms used can have some other properties so that our program can be more efficient and reasonable:

-  Fair

  We don't want one thread wait longer than others before they can enter critical section.

- Efficient

  We don't want processes to consume too much resources when they are waiting. For example, in many occasion we don't want to do busy waiting (spin wait). It would be better that we can relinquish CPU and let other processes run.

- Simple to use

  Yes it should be simple to use ![alt text](../img/icons/png/happy-4.png)

#### Using Lock to Avoid Critical Section

So, we now come to the main part of this essay: **implementing a lock to avoid critical section**

I believe you have known or heard of many different kinds of locks: [spin_lock](https://en.wikipedia.org/wiki/Spinlock), [rw_lock](https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock), [seq_lock](https://en.wikipedia.org/wiki/Seqlock) ... many of you know many variants of it, with different names and different implementations<sup>[2]</sup>. Basically locks are used in some situation like this:

```c
//process 1                   //process 2
lock();                       lock();
do_critical_section();        do_critical_region();
unlock();                     unlock();
```

What we are interested in is how can we implement a lock for this purpose. We have lots of choice, as long as our implementation meets all the criteria listed above. 

##### $\blacksquare$ Implementing lock by disabling interrupt

Implementation:

```
lock()                       unlock()
{                            {
  disable_interrupt();          enable_interrupt();
}                            }
```

This technique works pretty well on uniprocessor. On uniprocessor, the CPU can run **only one** process at a time. So by disabling interrupt, other processes would not have chances to run, let alone entering critical section.

The disadvantages, however, are obvious: it doesn't work on multiprocessor. And, moreover, by disabling interrupt, the process would probably take over the control of the whole computer from the operating system and might cause damage.

##### $\blacksquare$ Implementing software-based lock

By *software-based*, we mean that the lock can be implemented completely in software without help from hardware (e.g. interrupts). In this case, we can use *Peterson's algorithm*.

Let's go step by step.

##### ![alt text](../img/icons/svg/zap.svg) **1st attempt, A native implementation**

```c
int flag = 0;

lock() 
{  
  while(1 == flag)  // (1)
    ; //spin wait       
  flag = 1; //acquire the lock   
}

unlock()
{
  flag = 0; //release the lock
}
```

The idea is simple but naive: use one flag, test and set; if unavailable, do spin-wait.

However, there is one critical defect in this implementation: if both processes are at line (1) at the same time. In this occasion, they would both see `0 == flag` and proceed and set `flag=1`, which enable them to **enter critical section together**. That says, this implementation is **not safe**. Moreover, this implementation is not efficient because it use *spin waiting*, which is particularly bad on uniprocessor.

![alt text](../img/icons/svg/zap.svg) **2nd attempt, test and set**

```c
int flag[2] = {0, 0};

lock()
{
  flag[self] = 1;             //(1)
  while(1 == flag[1-self])    //(2)
    ; //spin wait
}

unlock()
{
  flag[self] = 0;
}
```

The idea is to use *per-thread flags*, **test and set**, to achieve mutual exclusion.

However, it would lead to [dead lock](https://en.wikipedia.org/wiki/Deadlock),  which means that no progress can make any process. To see why it can lead to dead lock, consider this procedure:

1. process 1 execute line (1)
2. process 2 execute line (1)
3. process 1 execute line (2), blocked
4. process 2 execute line (2), blocked

So, we have a dead lock here. It is **not live**, thus cannot meet the liveness criteria listed above.

![alt text](../img/icons/svg/zap.svg) **3rd attempt, strict alternation**

```c
//whose turn is it?
int turn = 0;

lock()
{
  //wait for my turn
  while(turn == 1 - self)
    ; //spin wait
}

unlock()
{
  //I am done. Your turn
  turn = 1 - self;
}
```

This is called **strict alternation**. It can be used to achieve mutual exclusion.

There is no dead lock anymore. However, there is one critical drawback: it is **not live** either because a thread can depend on threads outside critical section before it can enter critical section. To see why it is not live, imagine such a scenario:

```c
//process 1                   //process 2
while(true){                  while(true) {
  lock();                       lock(); 
  do_critical_region();         do_critical_region();
  unlock();                     unlock();
  do_extra_thing();
}
```

Assume that process 1 enter `lock()`, and then `do_critical_region()`, and then `unlock()`, and then `do_extra_thing()`, and then it continue to loop and going to enter `lock()`. What would happed now? **It is blocked even if process 2 is not running** because the variable `turn` is now set to `1-self`(meaning that it is not its turn). This is the reason why it is not live.

![alt text](../img/icons/svg/zap.svg) **4th attempt, Peterson's algorithm**

Now if we combine the ideas of **test-and-test**, **strict alternation** together, we have the *peterson's algorithm*.

```c
//whose turn is it?
int turn = 0;
//1: a thread/process wants to enter critical section,
//0: it doesn't
flag[2] = {0, 0};
lock()
{
  flag[self] = 1; //I need lock
  turn = 1 - self; //be polite, "push" other to run
  //wait for my turn
  while(flag[1-self] && turn == 1-self)
    ; //spin wait while the other thread has intent (to run)
      //AND it is the other thread's turn
}

unlock()
{
  //not anymore
  flag[self] = 0;
}
```

By using both **test and set** and **strict alternation**, we can meet all the criteria for avoiding race condition: safety, liveness, bounded wait.

But, for this algorithm to be correct, we do have to assume that every line of assignment operator is atomic, and the order of you code would not be re-ordered (which would cause problems on many modern out-of-order processors). And also, for N > 2 thread, you have to use another [Lamport's Bakery algorithm](https://en.wikipedia.org/wiki/Lamport%27s_bakery_algorithm). So, generally, this kind of lock is hard to implement and use.

##### $\blacksquare$ Implementing lock using special hardware instructions

With the help of special hardware instructions, we can implement a lock very easily:

Pseudo code:

```c
int flag = 0;
lock()                           unlock()
{                                {
  while(test_and_set(&flag))        flag = 0;
    ;                            
}                                }
```

The only thing we have to do is to use some special instructions to make this `test_and_set()` function atomic, *i.e.* when executed, it would not be stopped unless finished.

In other word, `test_and_set()` should perform the following codes atomically:

```c
int test_and_set(int *lock) {
  int old = *lock;
  *lock = 1;
  return old;
}
```

Fortunately, many modern processors provide this kind of instructions. For example, on x86, there is a **xchg** instruction, which, as far as I know, is used to implement most *spin locks* on many systems. Our `test_and_set()` can be implemented like this:

```c
long test_and_set(volatile long *lock)
{
  int old;
  asm("xchgl %0, %1"
     :"=r"(old), "+m"(*lock) //output
     :"0"(1)                 //input
     :"memory"               //can clobber anything in memory
     );
}
```

(note: instruction `xchg reg, addr` can atomically swaps *\*addr* and *reg*.  Go read some document if you cannot understand the assembly code above)

##### Summary

So far, we have tried three methods for implementing a lock:

1. Implement a lock by disabling interrupt
2. Implement a lock using completely software (*i.e.* peterson's algorithm)
3. Implement a lock using special hardware instructions (*e.g* `xchg` on X86)

Method 2) and 3) are quite successful to some extent. But, As you can see, they all adopt the same inefficient *busy waiting* routine: threads/processes "spins" around a lock until it is allowed to acquire it, which is not preferable in many context.

But, anyway, we have a lock here ![alt text](../img/icons/png/happy-4.png)

### \# 4, Locks That DON'T Spin/Busy waiting

Implementing a lock is easy, right?

Well, implementing a spin lock is easy. But spin lock itself comes with many disadvantages. First, it often waste CPU cycle. For example, imagine a thread holding a spin lock gets preempted and other threads try to acquire the same lock. In this case, those threads can do nothing but spin, which is useless. Cases got even complicated when it comes to multiprocessor. What would happen if a thread holding a lock gets preempted under this circumstance? well, lots of switching...

Of course spin locks [come with advantages](http://yarchive.net/comp/linux/semaphores.html)(search for *you need to grab a spinlock*). But generally speaking it would be better if we can have a locking mechanism that cause processes/threads to **yield CPU when locks are not available**. Put it another way, we would like the processes/threads to give up CPU *voluntarily* if lock are unavailable, rather than doing useless spinning.

Let's try.

Let's start with some pseudo code first for our **lock-that-dont-block**.

![alt text](../img/icons/svg/zap.svg) **1st attempt, A simple yield implementation**

```c
lock()
{
  while(test_and_set(&flag))
    yield();    //give up CPU and go to sleep...
}
```

Frankly this simple yield works, but it has some disadvantages:

1. There are lots of context switch coming with this kind of lock (*a.k.a* thunder herd)
2. It may cause starvation

The reason of the 1, 2 above is that there is no explicit control over who gets the lock next. So, we need explicit control here.

![alt text](../img/icons/svg/zap.svg) **2nd attempt, Simple yield with wait_queue**

```c
lock()
{
  while(test_and_set(&flag)) { //0
    add_myself_to_queue();  //1
    yield();                //2
  }
}

unlock()
{
  flag = 0;                       //3
  if(any_thread_in_wait_queue())  //4
    wake_up_one_wait_thread();
}
```

The idea here is to add thread to a queue when lock is unavailable; and in `unlock()` wake up one thread in queue, so that there would not be so many context switches and any starvation.

But, unfortunately there are two very severe problems:

1. **Lost wakeup**

   Assume that thread A hold the lock and thread B want it.

   - thread B execute line 0
   - thread B execute line 1, preparing to add itself to the wait queue
   - thread B got scheduled, and now thread A run
   - thread A unlock, and finding that there is no thread in the wait queue, it do nothing
   - After unlock, thread A finish, and now thread B continue to run
   - thread B continue to put itself to the wait queue and yield
   - Oops...no one here to wake up thread B...so there is a **lost wakeup**

2. **Wrong thread gets the lock**

   Actually at line 3 the lock is released. So, at this moment, any other thread can get the lock. Therefore even if some thread in the wait queue got waked up, it would probably find out that the lock has been taken by someone else.

So here comes to the real problem: how can we avoid **lost wakeup** and **wrong thread getting the lock** while at the same time save those context switches and starvation?

![alt text](../img/icons/svg/zap.svg) **3rd attempt, A decent yield implementation**<sup>[6]</sup>

```c
typedef struct __mutex_t {
  int flag;     //0: mutex is available, 1: mutex is not available
  int guard;    //guard lock to avoid losing wakeups
  queue_t *q;   //queue of waiting threads
} mutex_t;

void lock(mutex_t *m) {
  while (test_and_set(m->guard))
    ;  //acquire guard lock by spinning
  if(0 == m->flag) {
    m->flag = 1; //acquire mutex
    m->guard = 0;
  } else {
    //if the mutex has been taken, put itself into wait queue
    enqueue(m->q, self);
    m->guard = 0;
    yield();
  }
}

void unlock(mutex_t *m) {
  while(test_and_set(m->guard))
    ;
  if(queue_empty(m->q))
      //release mutex; no one wants mutex
      m->flag = 0;
  else
      //direct transfer mutex to next thread
      wakeup(dequeue(m->q));
  
  m->guard = 0;
}
```

I think anyone can understand this by comparison. In this implementation, we use an extra lock (`guard` in `__mutex_t`) to protect our internal data structure manipulation so that we are free from the race conditions listed above. So finally we have locks that don't  do busy waiting when the locks cannot be acquired. Instead, the process is put into a wait queue and sleep until other process holding the lock wake it up. In many places, it is more efficient than a spinlock.

Actually, this pseudo-implementation mimic many real world *semaphore* implementation. You will see in the following parts.

### \#5, Real World Locks ![alt text](../img/icons/png/linux-penguin.png)![alt text](../img/icons/png/freebsd-boy.png)![alt text](../img/icons/png/haiku-leaf.png)

Let's see how those operating systems (e.g. Linux, BSD) implement locks. Before we look into the codes, I would like to talk about some concepts first to lay a foundation.

#### Synchronization and Mutual Exclusion

There are lots of methods for avoiding race condition. For example, for single variable, we can use atomic operation. For more complicated scenario, we can use **locks**, **semaphore**, **monitors** and other IPC<sup>[3]</sup> mechanisms that are suitable for implementing *synchronization* and *mutual exclusion* between processes.

The later one (lock, semaphore, etc.) is more general. In summary, they can be classified into two classes: *Synchronization* and *Mutual Exclusion*. However, note that these two terms are often mixed together because they have similar meanings in many contexts. Indeed, sometimes synchronization can be achieved using mutual exclusion techniques. If you really can't distinguish these two concepts, it is probably fine to ignore the differences and treat them as a whole.

#### Lock, Spinlock, Semaphore, Mutex ... All The Same Things ?

It is fairly common to get messed up by those things and think that they are all the same or all different things. Actually, *Lock* is a very general term. Spinlock is a kind of lock that adopt the *busy waiting* approach. Semaphore is a variable or abstract data type used to control access to a common resource by **multiple** processes in a concurrent system such as a multiprogramming operating system<sup>[4]</sup>. It is a concurrency *primitive* that explicitly imply a *sleep/wakeup* behaviour<sup>[4]</sup>.Mutex stands for *mutual exclusion* and is a special kind of semaphore (binary semaphore). A spinlock is **not** a semaphore<sup>[4]</sup> because they have very different semantics (*e.g.* whether or not to allow multiple processes to enter critical section) and usually are implement very differently (*e.g.* whether or not spin/busy waiting). 

There are also many different kinds of locks (lock is a general term): *seq_lock*, *read-writer-lock*, *rcu_lock* ... They are all used (in different situations) to achieve some kind of mutual exclusion or synchronization.

Also, note that for the purpose of mutual exclusion and synchronization, there are also many IPC mechanism out there. You can use [*message queue*](https://en.wikipedia.org/wiki/Message_queue), [*FIFO*](https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)), [*signals*](https://en.wikipedia.org/wiki/Unix_signal), etc. They are not locks, but they provides us with the same function if used properly.

#### Spinlock in Linux<sup>[7]</sup>

Basically the spinlock implementation in the Linux kernel is like the **test-and-set** busy waiting routine stated above. But, besides from the test-and-set part, Linux use a fairer implementation called *ticket spinlock*. You see, the problem with the previous naive test-and-set spinlock is that it may cause unfair longer waiting for some processes/threads even if they tried to acquire the lock earlier than other, because there is no explicit record of their arriving time in this mechanism and therefore we do not know which process get blocked first and should be granted the lock first. To solve this issue, Linux use a neat trick: every time a process want to acquire the lock, it will be assigned a *ordered ticket* , so every process would finally acquire the lock by the order they arrived previously, which in effect is like a FIFO.

The two most important functions for us (now) would be `arch_spin_lock(`, which "spinlock" regarding to specific architectures and `arch_spin_unlock()`, which unlock the spinlock according to specific architectures, in ![alt tex](../img/icons/svg/file-code.svg)[spinlock.h](http://lxr.free-electrons.com/source/arch/x86/include/asm/spinlock.h?v=4.0#L142)<sup>[8]</sup>:

```c
 89 /*
 90  * Ticket locks are conceptually two parts, one indicating the current head of
 91  * the queue, and the other indicating the current tail. The lock is acquired
 92  * by atomically noting the tail and incrementing it by one (thus adding
 93  * ourself to the queue and noting our position), then waiting until the head
 94  * becomes equal to the the initial value of the tail.
 95  *
 96  * We use an xadd covering *both* parts of the lock, to increment the tail and
 97  * also load the position of the head, which takes care of memory ordering
 98  * issues and should be optimal for the uncontended case. Note the tail must be
 99  * in the high part, because a wide xadd increment of the low part would carry
100  * up and contaminate the high part.
101  */
102 static __always_inline void arch_spin_lock(arch_spinlock_t *lock)
103 {
104         register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };
105 
106         inc = xadd(&lock->tickets, inc); /* "assign" a ticket */
107         if (likely(inc.head == inc.tail))
108                 goto out;
109 
110         for (;;) {
111                 unsigned count = SPIN_THRESHOLD;
112 
                    /* constantly check whether it is ok to acquire the lock */
113                 do {
114                         inc.head = READ_ONCE(lock->tickets.head);
115                         if (__tickets_equal(inc.head, inc.tail)) /* test-and-set */
116                                 goto clear_slowpath;
117                         cpu_relax();
118                 } while (--count);
119                 __ticket_lock_spinning(lock, inc.tail);
120         }
121 clear_slowpath:
122         __ticket_check_and_clear_slowpath(lock, inc.head);
123 out:
124         barrier();      /* make sure nothing creeps before the lock is taken */
125 }
....
....
....
142 static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)
143 {
144         if (TICKET_SLOWPATH_FLAG &&
145                 static_key_false(&paravirt_ticketlocks_enabled)) {
146                 __ticket_t head;
147 
148                 BUILD_BUG_ON(((__ticket_t)NR_CPUS) != NR_CPUS);
149                 
  					/* increment the head of ticket (release the lock) */
150                 head = xadd(&lock->tickets.head, TICKET_LOCK_INC);
151 
152                 if (unlikely(head & TICKET_SLOWPATH_FLAG)) {
153                         head &= ~TICKET_SLOWPATH_FLAG;
154                         __ticket_unlock_kick(lock, (head + TICKET_LOCK_INC));
155                 }
156         } else
  					/* increment the head of ticket (release the lock) */
157                 __add(&lock->tickets.head, TICKET_LOCK_INC, UNLOCK_LOCK_PREFIX);
158 }
```

You see, every time a process want to acquire a spinlock, it atomically check to see whether the *head* and *tail* part of a ticket is equal (line 114~116). If they are equal, then nobody is holding the lock and it is safe to acquire it, otherwise, someone else is holding the lock and it have to wait (by spinning). In effect, the execution sequence is something like this:

<div align="center">![alt text](../img/linux-ticket-spinlock-illustration.png)</div>

In this case, process 2 will acquire the spinlock after process 1 release it, and process 3 after process 2, process 4 after process 3, ... , one by one. Please refer to the code if you don't understand this procedure. However, a few more things about the code: 

1. at line 106, the `xadd()` function **atomically** add *TICKET_LOCK_INC* (which is 1 in most cases) to the **tail** part of a ticket, meaning that "a ticket is assigned". But, note that `xadd()` returns the **previous** value!!! So, after the assignment

   ```c
   register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };
   ```

    `inc.tail` holds the previous value, NOT the one updated by *TICKET_LOCK_INC*. This `inc.tail` will NOT be updated in the `arch_spin_lock()` function. The current process would only constantly update `inc.head` (line 114) to check whether previous process has released the lock.

2. As an optimization, every process would spin for a *SPIN_THRESHOLD* time. After that, it will go into `__ticket_lock_spinning()` and be put into sleep ???(I guess)

That is the core of spinlock in Linux ![alt text](../img/icons/png/happy-4.png)

Actually, these two functions are only used internally and will not be exposed to the outside world. What Linux export are `spin_lock_init()`, `spin_lock()`, `spin_unlock()`, `spin_unlock_wait()`, `spin_is_locked()`, `spin_trylock()`, which are all macros<sup>[9]</sup>. Here is some description:

| Macro            | Actual Internal Implementation Function | Description                              |
| ---------------- | --------------------------------------- | ---------------------------------------- |
| spin_lock_init   |                                         | Initialize the lock (value 0)            |
| spin_lock        | __raw_spin_lock                         | Spinning until lock acquired (head == tail) |
| spin_unlock      | __raw_spin_unlock                       | Release the lock (head++)                |
| spin_unlock_wait | __raw_spin_unlock_wait                  | Do not want the lock, but spinning until the lock is released. |
| spin_is_locked   | __raw_spin_is_locked                    | Test whether the lock has been acquired  |
| spin_trylock     | __raw_spin_trylock                      | Acquire the lock if possible, else return with error |

As the spinlock implementation in Linux is quite huge and involve lots of architecture-specified issues, I would not analyze the complete implementation here. If you are interested in it, you can start by the `spin_lock()` [function/macro](http://lxr.free-electrons.com/source/include/linux/spinlock.h?v=4.0#L310) and `spin_lock_init()` [function/macro](http://lxr.free-electrons.com/source/include/linux/spinlock.h?v=4.0#L304) and take a travel yourself. Enjoy~![alt text](../img/icons/png/happy-4.png)

For more information, you can also refer to this [Github article](https://0xax.gitbooks.io/linux-insides/content/SyncPrim/sync-1.html), or do a Google search of "ticket spinlock linux" youself !

#### Queued Spinlock in Linux

Ticket spinlock is good. Simple and efficient. But, there can still be some improvement. Remember that Linux use a "public" ticket shared by all processes to ensure global ordering of lock acquisition. However, with different threads trying to check the ticket using the **test-and-set** instruction, there would definitely be lots of [cache invalidation](https://en.wikipedia.org/wiki/Cache_invalidation) because **test-and-set** will keep writing 1 to the variable and see if there would a 0 returned. And since different processors use different cache, they have to invalidate their cache to ensure that all the processors see the same value of a variable (the ticket).

To eliminate this problem, the Linux kernel by default adopt a spinlock mechanism called *queued spinlock*, which enables every process to spin on their own local(per-cpu) variable. The idea is simple, and it is based on the [MCS](http://www.cs.rochester.edu/~scott/papers/1991_TOCS_synch.pdf) locking mechanism from the 1990s. 

However, as I only want to illustrate the locking mechanism (in general) but not every detail, I would not analyze it here. Please do a research yourself If you are interested ![alt text](../img/icons/png/happy-4.png)

#### Semaphore in Linux

Let's look into the semaphore implementation in the Linux  kernel. The source can be found in [include/linux/semaphore.h](http://lxr.free-electrons.com/source/include/linux/semaphore.h) and [kernel/locking/semaphore.c](http://lxr.free-electrons.com/source/kernel/locking/semaphore.c) in the Linux kernel source code.

Unlike spinlock, semaphore is a more "general" locking mechanism that allows multiple processes to enter the critical section. In most implementation, it does not adopt the "spinning/busy-waiting" routine, but instead put a process that fails to "acquire" a semaphore into a wait queue and wake it up when the semaphore is ready. By this design, it avoid wasting CPU time by spinning. However, as using semaphore requires explicit task scheduling, it has more overhead than using spinlock<sup>[10]</sup>.

The implementation of semaphore in Linux is quite simple, well-documented by comments and mimic that presented in the **A simple yield implementation** above. So I would copy-paste the whole implementation here directly.  If you are a programmer(or anything else) who want more insight into the mechanism of it, please take some time and look into the source code carefully. It is not that hard:

![alt tex](../img/icons/svg/file-code.svg)**semaphore.h**

<div class="codeblks"	>

```c
/*
 * Copyright (c) 2008 Intel Corporation
 * Author: Matthew Wilcox <willy@linux.intel.com>
 *
 * Distributed under the terms of the GNU GPL, version 2
 *
 * Please see kernel/semaphore.c for documentation of these functions
 */
#ifndef __LINUX_SEMAPHORE_H
#define __LINUX_SEMAPHORE_H

#include <linux/list.h>
#include <linux/spinlock.h>

/* Please don't access any members of this structure directly */
struct semaphore {
	raw_spinlock_t		lock;
	unsigned int		count;
	struct list_head	wait_list;
};

#define __SEMAPHORE_INITIALIZER(name, n)				\
{									\
	.lock		= __RAW_SPIN_LOCK_UNLOCKED((name).lock),	\
	.count		= n,						\
	.wait_list	= LIST_HEAD_INIT((name).wait_list),		\
}

#define DEFINE_SEMAPHORE(name)	\
	struct semaphore name = __SEMAPHORE_INITIALIZER(name, 1)

static inline void sema_init(struct semaphore *sem, int val)
{
	static struct lock_class_key __key;
	*sem = (struct semaphore) __SEMAPHORE_INITIALIZER(*sem, val);
	lockdep_init_map(&sem->lock.dep_map, "semaphore->lock", &__key, 0);
}

extern void down(struct semaphore *sem);
extern int __must_check down_interruptible(struct semaphore *sem);
extern int __must_check down_killable(struct semaphore *sem);
extern int __must_check down_trylock(struct semaphore *sem);
extern int __must_check down_timeout(struct semaphore *sem, long jiffies);
extern void up(struct semaphore *sem);

#endif /* __LINUX_SEMAPHORE_H */
```

</div>

This header file is for semaphore initialization and exporting function. The main logic is implemented in another file, as follow:

![alt tex](../img/icons/svg/file-code.svg)**semaphore.c**

<div class="codeblks">

```c
/*
 * Copyright (c) 2008 Intel Corporation
 * Author: Matthew Wilcox <willy@linux.intel.com>
 *
 * Distributed under the terms of the GNU GPL, version 2
 *
 * This file implements counting semaphores.
 * A counting semaphore may be acquired 'n' times before sleeping.
 * See mutex.c for single-acquisition sleeping locks which enforce
 * rules which allow code to be debugged more easily.
 */

/*
 * Some notes on the implementation:
 *
 * The spinlock controls access to the other members of the semaphore.
 * down_trylock() and up() can be called from interrupt context, so we
 * have to disable interrupts when taking the lock.  It turns out various
 * parts of the kernel expect to be able to use down() on a semaphore in
 * interrupt context when they know it will succeed, so we have to use
 * irqsave variants for down(), down_interruptible() and down_killable()
 * too.
 *
 * The ->count variable represents how many more tasks can acquire this
 * semaphore.  If it's zero, there may be tasks waiting on the wait_list.
 */

#include <linux/compiler.h>
#include <linux/kernel.h>
#include <linux/export.h>
#include <linux/sched.h>
#include <linux/sched/debug.h>
#include <linux/semaphore.h>
#include <linux/spinlock.h>
#include <linux/ftrace.h>

static noinline void __down(struct semaphore *sem);
static noinline int __down_interruptible(struct semaphore *sem);
static noinline int __down_killable(struct semaphore *sem);
static noinline int __down_timeout(struct semaphore *sem, long timeout);
static noinline void __up(struct semaphore *sem);

/**
 * down - acquire the semaphore
 * @sem: the semaphore to be acquired
 *
 * Acquires the semaphore.  If no more tasks are allowed to acquire the
 * semaphore, calling this function will put the task to sleep until the
 * semaphore is released.
 *
 * Use of this function is deprecated, please use down_interruptible() or
 * down_killable() instead.
 */
void down(struct semaphore *sem)
{
	unsigned long flags;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0))
		sem->count--;
	else
		__down(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
EXPORT_SYMBOL(down);

/**
 * down_interruptible - acquire the semaphore unless interrupted
 * @sem: the semaphore to be acquired
 *
 * Attempts to acquire the semaphore.  If no more tasks are allowed to
 * acquire the semaphore, calling this function will put the task to sleep.
 * If the sleep is interrupted by a signal, this function will return -EINTR.
 * If the semaphore is successfully acquired, this function returns 0.
 */
int down_interruptible(struct semaphore *sem)
{
	unsigned long flags;
	int result = 0;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0))
		sem->count--;
	else
		result = __down_interruptible(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);

	return result;
}
EXPORT_SYMBOL(down_interruptible);

/**
 * down_killable - acquire the semaphore unless killed
 * @sem: the semaphore to be acquired
 *
 * Attempts to acquire the semaphore.  If no more tasks are allowed to
 * acquire the semaphore, calling this function will put the task to sleep.
 * If the sleep is interrupted by a fatal signal, this function will return
 * -EINTR.  If the semaphore is successfully acquired, this function returns
 * 0.
 */
int down_killable(struct semaphore *sem)
{
	unsigned long flags;
	int result = 0;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0))
		sem->count--;
	else
		result = __down_killable(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);

	return result;
}
EXPORT_SYMBOL(down_killable);

/**
 * down_trylock - try to acquire the semaphore, without waiting
 * @sem: the semaphore to be acquired
 *
 * Try to acquire the semaphore atomically.  Returns 0 if the semaphore has
 * been acquired successfully or 1 if it it cannot be acquired.
 *
 * NOTE: This return value is inverted from both spin_trylock and
 * mutex_trylock!  Be careful about this when converting code.
 *
 * Unlike mutex_trylock, this function can be used from interrupt context,
 * and the semaphore can be released by any task or interrupt.
 */
int down_trylock(struct semaphore *sem)
{
	unsigned long flags;
	int count;

	raw_spin_lock_irqsave(&sem->lock, flags);
	count = sem->count - 1;
	if (likely(count >= 0))
		sem->count = count;
	raw_spin_unlock_irqrestore(&sem->lock, flags);

	return (count < 0);
}
EXPORT_SYMBOL(down_trylock);

/**
 * down_timeout - acquire the semaphore within a specified time
 * @sem: the semaphore to be acquired
 * @timeout: how long to wait before failing
 *
 * Attempts to acquire the semaphore.  If no more tasks are allowed to
 * acquire the semaphore, calling this function will put the task to sleep.
 * If the semaphore is not released within the specified number of jiffies,
 * this function returns -ETIME.  It returns 0 if the semaphore was acquired.
 */
int down_timeout(struct semaphore *sem, long timeout)
{
	unsigned long flags;
	int result = 0;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0))
		sem->count--;
	else
		result = __down_timeout(sem, timeout);
	raw_spin_unlock_irqrestore(&sem->lock, flags);

	return result;
}
EXPORT_SYMBOL(down_timeout);

/**
 * up - release the semaphore
 * @sem: the semaphore to release
 *
 * Release the semaphore.  Unlike mutexes, up() may be called from any
 * context and even by tasks which have never called down().
 */
void up(struct semaphore *sem)
{
	unsigned long flags;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(list_empty(&sem->wait_list)))
		sem->count++;
	else
		__up(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
EXPORT_SYMBOL(up);

/* Functions for the contended case */

struct semaphore_waiter {
	struct list_head list;
	struct task_struct *task;
	bool up;
};

/*
 * Because this function is inlined, the 'state' parameter will be
 * constant, and thus optimised away by the compiler.  Likewise the
 * 'timeout' parameter for the cases without timeouts.
 */
static inline int __sched __down_common(struct semaphore *sem, long state,
								long timeout)
{
	struct semaphore_waiter waiter;

	list_add_tail(&waiter.list, &sem->wait_list);
	waiter.task = current;
	waiter.up = false;

	for (;;) {
		if (signal_pending_state(state, current))
			goto interrupted;
		if (unlikely(timeout <= 0))
			goto timed_out;
		__set_current_state(state);
		raw_spin_unlock_irq(&sem->lock);
		timeout = schedule_timeout(timeout);
		raw_spin_lock_irq(&sem->lock);
		if (waiter.up)
			return 0;
	}

 timed_out:
	list_del(&waiter.list);
	return -ETIME;

 interrupted:
	list_del(&waiter.list);
	return -EINTR;
}

static noinline void __sched __down(struct semaphore *sem)
{
	__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
}

static noinline int __sched __down_interruptible(struct semaphore *sem)
{
	return __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
}

static noinline int __sched __down_killable(struct semaphore *sem)
{
	return __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);
}

static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)
{
	return __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);
}

static noinline void __sched __up(struct semaphore *sem)
{
	struct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,
						struct semaphore_waiter, list);
	list_del(&waiter->list);
	waiter->up = true;
	wake_up_process(waiter->task);
}
```

</div>

Hmm...lots of functions here. Actually they all provides similar functionalities(*e.g.* interruptable or non-interruptable). Let's take `down_interruptable()` for example and analyze it a little bit:

```c
/*
 * the execute seqeunce here would be 
 *    down_interruptible()  -> __down_interruptable() -> __down_common()
 */
int down_interruptible(struct semaphore *sem)
{
	unsigned long flags;
	int result = 0;

    /* use a lock to protect internal data structure manipulation,
     * like the `guard' variable in "A simple yield implementation" above
	 * Also see footnote [5] for the "irq" issue
     */
	raw_spin_lock_irqsave(&sem->lock, flags);  
	if (likely(sem->count > 0))
		sem->count--;    /* if there are still count > 0, down is ok. Simply dec it */
	else
		result = __down_interruptible(sem); /*current process not allowed to enter
                                             * critical section. It would be placed 
                                             * into the wait queue afterwards
                                             */
	raw_spin_unlock_irqrestore(&sem->lock, flags);

	return result;
}

static noinline int __sched __down_interruptible(struct semaphore *sem)
{
	return __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
}

static inline int __sched __down_common(struct semaphore *sem, long state,
								long timeout)
{
	struct semaphore_waiter waiter;
     
    /* add current process to the wait queue */
	list_add_tail(&waiter.list, &sem->wait_list); 
	waiter.task = current;
	waiter.up = false;

  /* 
   * after putting the current process into the wait queue, give up
   * CPU to the scheduler (by `schedule_timeout()`) and loop forever
   * until 
   *   1) timeout or
   *   2) interrupted or
   *   3) someone wait it up (indicated by `waiter.up`)
   */
	for (;;) {
		if (signal_pending_state(state, current)) /* interrupted */
			goto interrupted;  /* "Dijkstra must hate me" --Linus */
		if (unlikely(timeout <= 0))
			goto timed_out;
		__set_current_state(state);
		raw_spin_unlock_irq(&sem->lock);
		timeout = schedule_timeout(timeout);
		raw_spin_lock_irq(&sem->lock);
		if (waiter.up)
			return 0;
	}

 timed_out:
	list_del(&waiter.list);
	return -ETIME;

 interrupted:
	list_del(&waiter.list);
	return -EINTR;
}
```

This implementation mimics that in **A decent yield implementation** above (except that semaphore allows multiple processes to enter critical section). For example, in `down_interruptible()`, the `raw_spin_lock_irqsave(&sem->lock, flags);` is used for the same purpose as `while (test_and_set(m->guard));` in **A decent yield implementation**: to protect the internal data structure of a semaphore/lock; and the usage of `semaphore.wait_list` is pretty much the same as `__mutex_t.queue_t`. (the only thing that requires more attention is that the `raw_spin_lock_irqsave(&sem->lock, flags)` will also disable **local** interrupt<sup>[5]</sup>)

Hopefully you can understand this (read the source code carefully please). If you want a more direct implementation(lots of assembly), you can check the implementation in Linux 2.4 [here](https://www.kernel.org/pub/linux/kernel/people/marcelo/linux-2.4/include/asm-i386/semaphore.h) and [here](https://www.kernel.org/pub/linux/kernel/people/marcelo/linux-2.4/arch/i386/kernel/semaphore.c), in which you can find codes without so many jumps.

#### Locking in FreeBSD

Let's look into FreeBSD and see how it do for spinlock ![alt text](../img/icons/png/happy-4.png)

(I am still investigating on the FreeBSD implementation. But since the semantic of locking in FreeBSD is pretty different from that of Linux (e.g. FreeBSD allow *recursive locking* ), and I not so familiar with the codebase, it will take sometime. Thanks for your reading so far :)

#### Locking in Haiku

[Haiku](https://www.haiku-os.org/) is an open source operating system with a nice design and a user friendly interface. It stems from the BeOS in the 2000s. I currently involved in a [GSoC](https://developers.google.com/open-source/gsoc/) project for Haiku. So, let's see what Haiku-related contents I can add here.



###### ![alt text](../img/icons/svg/search.svg) Notes

-------------------------

<div class="notes">

1. Some concepts/terminologies stated here may or may not be strictly adhered to the theory (I sometimes mix concepts). And also note that is this essay I will use the term *thread* and *processe* interchangeably, as they behave similarly in this context.

2. Please note that there are many different implementations of a particular kind of lock. For example, if you wish, you can implement `pthread_mutex_lock()` as a spinlock.

3. IPC stands for Inter-process communication

4. Linux Kernel Mailing List, [*Re: NT kernel guy playing with Linux*](http://yarchive.net/comp/linux/semaphores.html), by Linus Torvalds

5. On both SMP and UP system, lock should **not** be used in an interrupt handler, because that would probably cause deadlock. For example, consider:

   ```c
   lock(&somelock);
   ...
     <- interrupt comes in
     	lock(&somelock);    <- lock in interrupt handler
   ```

    If the interrupt handler is waken up on a different CPU, it ok. But it is **not** ok if it happen on the same CPU that holds the lock, because the lock will obviously never be released (the interrupt is waiting for the lock, and the lock-holder is interrupted by the interrupt and will not continue until the interrupt has been processed). For this reason, the Linux kernel provides some other safer lock primitives that disable **local** interrupt (*e.g.* `spin_lock_irqsave()`, `read_lock_irqsave()`, `write_lock_irqsave()`).

   Note also that context switch in effect requires interrupt ![alt text](../img/icons/png/happy-7.png)

6. Many thanks to one [course outline](http://www.cs.columbia.edu/~junfeng/11sp-w4118/lectures/l09-lock.pdf) by *Junfeng Yang* at Columbia University, with the help of which I start to understand the internal mechanism used by many modern operating system.

7. As the kernel evolves, it used a quite different implementation from *ticket spinlock*. Please see [my another essay](http://walkerlala.github.io/archive/modern-linux-locking.html) for more information.

8. For now, Let's consider x86 only.

9. I saw lots of macros used in the Linux kernel source, even in places where a simple while more readable function is sufficient. I was wondering what this kind of usage of macros would bring us. Efficiency as you don't have to do so many function jumps? I don't know. If you have any idea, please leave a comment or [drop me a line](mailto:ablacktshirt@gmail.com).

10. Note also since using spinlock require disabling interrupt first and enabling it afterwards, there is also some overhead with spinlock


</div>




